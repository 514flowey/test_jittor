{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_file(file_ID_name, file_result_name):\n",
    "    file_ID = open(file_ID_name, \"r\")\n",
    "    file_result = open(file_result_name, \"r\")\n",
    "    data = []\n",
    "    \n",
    "    for line1, line2 in zip(file_ID, file_result):\n",
    "        feature = line1.replace('\\n','').lower().split(' ')\n",
    "        label = line2.replace('\\n','').lower().split(',')\n",
    "        label = [int(x) for x in label]\n",
    "        data.append([feature, label[1:]])\n",
    "    \n",
    "    random.shuffle(data)\n",
    "    file_ID.close()\n",
    "    file_result.close()\n",
    "    return data\n",
    "\n",
    "folder_name = \"/.cached/data/\"\n",
    "train_ID_name = folder_name + \"ID_train\"\n",
    "train_result_name = folder_name + \"ISEAR_train\"\n",
    "train_data = decode_file(train_ID_name, train_result_name)\n",
    "\n",
    "test_ID_name = folder_name + \"ID_test\"\n",
    "test_result_name = folder_name + \"ISEAR_test\"\n",
    "test_data = decode_file(train_ID_name, test_result_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVocab:\n",
    "    def __init__(self):\n",
    "        self._vocab = {}\n",
    "        self._size = 1\n",
    "        \n",
    "    def insert(self, word):\n",
    "        if word not in self._vocab.keys():\n",
    "            self._vocab[word] = self._size\n",
    "            self._size = self._size + 1\n",
    "    \n",
    "    def locate(self, word):\n",
    "        if word not in self._vocab.keys():\n",
    "            return 0\n",
    "        return self._vocab[word]\n",
    "    \n",
    "    def size(self):\n",
    "        return self._size\n",
    "\n",
    "def build_vocab(data):\n",
    "    vocab = MyVocab()\n",
    "    for sentence in data:\n",
    "        for word in sentence[0]:\n",
    "            vocab.insert(word)\n",
    "    return vocab\n",
    "\n",
    "def resize_sentence(data, normal_len):\n",
    "    def pad(sentence, size):\n",
    "        return sentence[:size] if len(sentence) > size else sentence+[0]*(size-len(sentence))\n",
    "    return [[pad(sentence[0], normal_len), sentence[1]] for sentence in data]\n",
    "\n",
    "def build_features_and_labels(vocab, data):\n",
    "    data = resize_sentence(data, 180)\n",
    "    features = []\n",
    "    labels = []\n",
    "    for sentence in data:\n",
    "        features.append([vocab.locate(x) for x in sentence[0]])\n",
    "        labels.append(np.argmax(sentence[1][1:]))\n",
    "    return torch.tensor(features), torch.tensor(labels)\n",
    "\n",
    "train_vocab = build_vocab(train_data)\n",
    "train_features, train_labels = build_features_and_labels(train_vocab, train_data)\n",
    "test_features, test_labels = build_features_and_labels(train_vocab, test_data)\n",
    "train_set = Data.TensorDataset(train_features, train_labels)\n",
    "test_set = Data.TensorDataset(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Data.TensorDataset(train_features, train_labels)\n",
    "test_set = Data.TensorDataset(train_features, train_labels)\n",
    "\n",
    "batch_size = 13\n",
    "train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_set, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, num_emotions=7):\n",
    "        '''\n",
    "        @params:\n",
    "            vocab: 在数据集上创建的词典，用于获取词典大小\n",
    "            embed_size: 嵌入维度大小\n",
    "            num_hiddens: 隐藏状态维度大小\n",
    "            num_layers: 隐藏层个数\n",
    "        '''\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # encoder-decoder framework\n",
    "        # bidirectional设为True即得到双向循环神经网络\n",
    "        self.encoder = nn.LSTM(input_size=embed_size, \n",
    "                                hidden_size=num_hiddens, \n",
    "                                num_layers=num_layers,\n",
    "                                bidirectional=True)\n",
    "        self.decoder = nn.Linear(4*num_hiddens, num_emotions) # 初始时间步和最终时间步的隐藏状态作为全连接层输入\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        @params:\n",
    "            inputs: 词语下标序列，形状为 (batch_size, seq_len) 的整数张量\n",
    "        @return:\n",
    "            outs: 对文本情感的预测，形状为 (batch_size, 2) 的张量\n",
    "        '''\n",
    "        # 因为LSTM需要将序列长度(seq_len)作为第一维，所以需要将输入转置\n",
    "        embeddings = self.embedding(inputs.permute(1, 0)) # (seq_len, batch_size, d)\n",
    "        # rnn.LSTM 返回输出、隐藏状态和记忆单元，格式如 outputs, (h, c)\n",
    "        outputs, _ = self.encoder(embeddings) # (seq_len, batch_size, 2*h)\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), -1) # (batch_size, 4*h)\n",
    "        outs = self.decoder(encoding) # (batch_size, 2)\n",
    "        return outs\n",
    "\n",
    "embed_size, num_hiddens, num_layers = 100, 100, 2\n",
    "net = BiRNN(train_vocab.size(), embed_size, num_hiddens, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        device = list(net.parameters())[0].device \n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                net.eval()\n",
    "                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "                net.train()\n",
    "            else:\n",
    "                if('is_training' in net.__code__.co_varnames):\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "\n",
    "def train(train_iter, test_iter, net, loss, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    batch_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y) \n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cpu\n",
      "epoch 1, loss 1.6395, train acc 0.347, test acc 0.525, time 129.2 sec\n",
      "epoch 2, loss 0.6162, train acc 0.533, test acc 0.696, time 130.0 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.01, 5\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "train(train_iter, test_iter, net, loss, optimizer, torch.device('cpu'), num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
